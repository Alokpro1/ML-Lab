{"cells":[{"source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATASETS\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE CELL.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\n\nCHUNK_SIZE = 40960 \nDATASET_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1069%252F1940%252Fbundle%252Farchive.zip%3FGoogleAccessId%3Dgcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%26Expires%3D1600522018%26Signature%3DJvDbLDN2je6WG2P9pqF%252FnY0oanFxrXuLVHvygqjCFknqiMkuLRQ%252ByvD9EnlxwcvSPrXWT5VSxX87ZPQX0rRS5E03UrQ6nYmCHKSVVQO3rJob1mBDS1SUm3XoOlOjigffxwHUpK7rD69PDP8mICMIq8TPOaelPOFyr6O67DFkuDTtbc8t8AKdIjdLYtq5x7GRY7tQpatA7h0ouLhJjHTy6VwnldZvVLRw96YE6XSwU%252F%252FWT1MdbZW144tTiJ2wFcnUoXnNBiU8gX5s1jD1lLOfFMdJ2GRFY3FF6oqrWdGQM%252BzSXNcgTj4V0M5AexUrWjGUAQ1e1ePOdinzLnGbKpQEZw%253D%253D'\nKAGGLE_INPUT_PATH='/home/kaggle/input'\nKAGGLE_INPUT_SYMLINK='/kaggle'\n\n!mkdir -p -- $KAGGLE_INPUT_PATH\n!chmod 777 $KAGGLE_INPUT_PATH\n!ln -sfn $KAGGLE_INPUT_PATH ../\n!mkdir -p -- $KAGGLE_INPUT_SYMLINK\n!ln -sfn $KAGGLE_INPUT_PATH $KAGGLE_INPUT_SYMLINK\n\nfor dataset_mapping in DATASET_MAPPING.split(','):\n    directory, download_url_encoded = dataset_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as zipfileres, NamedTemporaryFile() as tfile:\n            total_length = zipfileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes zipped')\n            dl = 0\n            data = zipfileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = zipfileres.read(CHUNK_SIZE)\n            print(f'\\nUnzipping {directory}')\n            with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\nprint('Dataset import complete.')\n","metadata":{},"cell_type":"code"},{"metadata":{"_uuid":"bd49ff49425165a146a9db9a89408468daf1e0b5"},"cell_type":"markdown","source":"# Classifying Fraudulent and Valid Transactions"},{"metadata":{"_uuid":"390941484e81e5f088413b114938ae7c09765cb3"},"cell_type":"markdown","source":"## Goals of This Project.\n1. Classify fraudulent transactions as accurately as possible.\n2. Learn how to explore data, pre-process it, apply multiple popular machine learning methods and diagnose their performances (or lack thereof)"},{"metadata":{"_uuid":"7efd284cfb4693f7a5d2e7d3c23780c613776904"},"cell_type":"markdown","source":"<a id='top'></a>\n## Overview: \n### 0. <b><a href='#brief'>Brief Description of Dataset</a></b>\n0.1 <a href='#Load'> Loading Data</a>\n\n### 1. <b><a href='#EDA'>Exploratory Data Analysis</a></b>\n\n1.1. <a href='#SummData'>Summary of Dataset </a>\n\n1.2. <a href='#KeyTake'>Key Takeaways</a>\n\n1.3. <a href='#AcctType'>Looking At Account Types</a>\n\n1.4. <a href='#TransType'>Looking At Transaction Types</a>\n\n1.5. <a href='#bal'>Looking at balances before and after transactions</a>\n\n1.6. <a href='#AcctandTrans'>Another Look at Transaction Types and Account Names</a>\n\n1.7. <a href='#Flag'>Looking At Flagged Transactions</a>\n\n1.8. <a href='#Time'>Looking At Time</a>\n\n1.9. <a href='#Amt'>Looking At Amounts Moved in Transactions</a>\n\n### 2. <b><a href='#Preprocess'>Pre-processing Data</a></b>\n2.1. <a href='#Cat'>Handling Categorical Variables</a>\n\n2.2. <a href='#Split'> Splitting/Standardizing Data</a>\n\n### 3. <b><a href='#Models'>Model Selection</a></b>\n3.1. <a href='#Model-1'>Artificial Neural Networks</a>\n\n3.2. <a href='#Model-2'>Random Forests</a>\n\n3.3. <a href='#Model-3'>Extreme Gradient Boosting</a>\n\n3.4. <a href='#Visuals'>Comparing Performances</a>\n\n\n### 4. <b><a href='#Final'>Final Remarks</a></b>\n"},{"metadata":{"_uuid":"f9c307c75bae9a5b66f05d01c015937455400a6d"},"cell_type":"markdown","source":"## 0. Brief Description of Dataset\n\n<a id='brief'></a>\nInformation about dataset, paysim1: \n\nURL of Kaggle Dataset page: https://www.kaggle.com/ntnu-testimon/paysim1\n\nThis dataset was a sample of a much larger dataset (not available on Kaggle) generated from a simulation that closely resembles the normal day-to-day transactions including the occurrence of fraudulent transactions.\n\nThe dataset was made for performing research on fraud detection methods.\n\nHere are the variables in the datasets as well as their descriptions: \n\n- **step** - *integer* - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\n\n- **type** - *string/categorical* - type of transaction: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n\n- **amount** - *float* - amount of the transaction in local currency.\n\n- **nameOrig** - *string* - customer who initiated the transaction\n\n- **oldbalanceOrg** - *float* initial balance before the transaction\n\n- **newbalanceOrig** - *float* - new balance after the transaction\n\n- **nameDest** - *string* - customer who is the recipient of the transaction\n\n- **oldbalanceDest** - *float* - initial balance of recipient before the transaction.\n\n- **newbalanceDest** - *float* - new balance of recipient after the transaction.\n\n- **isFraud** - *boolean/binary* - determines if transaction is fraudulent (encoded as 1) or valid (encoded as 0)\n\n- **isFlaggedFraud** - *boolean/binary* - determines if transaction is flagged as fraudulent (encoded as 1) or not flagged at all (encoded as 0). An observation is flagged if the transaction is fraudulent and it involved a transfer of over 200,000 in the local currency."},{"metadata":{"_uuid":"5543352ca57de12f5bd8ec564e47f36881b89a56"},"cell_type":"markdown","source":"<a href='#menu'>go back to top</a>"},{"metadata":{"_uuid":"fd376936ce69ca28360a80814a602b9191ac9dca"},"cell_type":"markdown","source":"### 0.1 Loading Data\n<a id='Load'>"},{"metadata":{"trusted":false,"_uuid":"5d7381e5a304f419c0ea2119d90e2d793f697766"},"cell_type":"code","source":"# loading needed methods\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.cm as cm\nfrom random import seed,sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score, roc_curve, auc,\\\nprecision_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"71479e82537154fac60290ccb882f2ec1d7c778b"},"cell_type":"code","source":"# loading data\n\ndata = pd.read_csv(\"../input/PS_20174392719_1491204439457_log.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa10abb1993826e69621841e99fbfc7b926eb09"},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis\n<a id='EDA'></a>"},{"metadata":{"_uuid":"2d58cf9086c0ce2f3f888cdba2aa6a812a10a1a9"},"cell_type":"markdown","source":"### 1.1 Summary of Dataset\n<a id='SummData'></a>"},{"metadata":{"trusted":false,"_uuid":"44dbc09b37da0a5abc23d3d15c1129ddde5ae5ea"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"eb9ae3a5fa40c19f93297aa82d405a1bd8350e63"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a1956bb59911cfba3e6df5727f789ca05732c4c5"},"cell_type":"code","source":"data.head(7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce767ad109557c6f1f98bbc61d61764cd8bb88be"},"cell_type":"markdown","source":"### 1.2 Key Takeaways:\n<a id='KeyTake'></a>\n1. There are no missing values\n2. There are just over 6 million observations\n3. There are 11 variables\n4. Most transactions involve amounts less than 1 million euros.\n5. Most observations in the dataset are of valid transactions, so any patterns related to identifying fraud transactions may be hard to see, data is also unbalanced.\n6. From the sample of observations, there are many instances where what happens to the recipient account (oldbalanceDest, newbalanceDest) does not make sense (e.g. the very first observation involved a payment of 9839.64 yet, the balance before and after the transaction equals 0.) \n\n"},{"metadata":{"_uuid":"b06d6ebfaedac476017a02ff43e5d5ce8b73e5ca"},"cell_type":"markdown","source":"### Responding to takeaways\n\n1. No imputation is required until further notice\n2. Non-parametric machine learning methods may be preferred due to the large size of the data and that the goal is accurate classification, not interpretation\n3. Dimension Reduction methods may not be necessary\n4. (and points 5, 6) Since the data is unbalanced I want to visually compare fraud transactions to valid transactions and see if there are any important patterns that could be useful.    \n\n"},{"metadata":{"_uuid":"1a8cae29590ac3810a581695838532c5a68d3769"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"faaa730c3625768c574b487b99e74ae4e6d26a65"},"cell_type":"markdown","source":"### 1.3 Looking at Account Types \n<a id='AcctType'></a>"},{"metadata":{"_uuid":"dbc73f8304a9ca9d9bd76d78e5ebc77672a2f266"},"cell_type":"markdown","source":"One feature of the dataset that is not immediately presented on the kaggle overview page is the account types \"C\" (customer) and \"M\", which would be the first character for each value under nameOrig and nameDest. Could this be a predictor?\n\nI will create a feature \"type1\" which is a categorical variable with levels \"CC\" (Customer to Customer), \"CM\" (Customer to Merchant), \"MC\" (Merchant to Customer), \"MM\" (Merchant to Merchant)."},{"metadata":{"trusted":false,"_uuid":"6010843c4f59c40e7e4d11d7c3630625c0b57dea"},"cell_type":"code","source":"# adding feature type1\ndata_new = data.copy() # creating copy of dataset in case I need original dataset\ndata_new[\"type1\"] = np.nan # initializing feature column\n\n# filling feature column\ndata_new.loc[data.nameOrig.str.contains('C') & data.nameDest.str.contains('C'),\"type1\"] = \"CC\" \ndata_new.loc[data.nameOrig.str.contains('C') & data.nameDest.str.contains('M'),\"type1\"] = \"CM\"\ndata_new.loc[data.nameOrig.str.contains('M') & data.nameDest.str.contains('C'),\"type1\"] = \"MC\"\ndata_new.loc[data.nameOrig.str.contains('M') & data.nameDest.str.contains('M'),\"type1\"] = \"MM\"\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c96ca747b11c00eb2b7c8eae7e686b2927da9185"},"cell_type":"markdown","source":"In the next few code cells, I seek to compare valid transactions against fraud transactions instead of overall trends.\n\nThis is because I want to see patterns that differentiate fraud transactions from valid ones."},{"metadata":{"trusted":false,"_uuid":"0d11336305c5897c1569854023135f5e988b9950"},"cell_type":"code","source":"# Subsetting data into observations with fraud and valid transactions:\nfraud = data_new[data_new[\"isFraud\"] == 1]\nvalid = data_new[data_new[\"isFraud\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c4a54ce2d3ed6115b7af7c6fe9c52e40b32dcd6"},"cell_type":"code","source":"# seeing the counts of transactions by type1 (CC,CM,MC,MM)\nprint(\"Fraud transactions by type1: \\n\",fraud.type1.value_counts())\nprint(\"\\n Valid transactions by type1: \\n\",valid.type1.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3437a7f2ea69f9ea262e6a4e88733f567bb260b"},"cell_type":"markdown","source":"#### Conclusion: \n\nFrom the dataset, it seems that fraud transactions only occur when the transaction type1 is CC (Customer to Customer)\n\nSince the dataset is sample of the population, I would have resampled the data to see if this phenomenon held.\n\nSince I do not have access to the population, I will assume that transaction only occur when transaction type1 is CC.\n\nThis also means that the datasets fraud and valid don't need to be subsetted. However, since all relevant observations have type1 = \"CC\", the type1 column is no longer necessary."},{"metadata":{"trusted":false,"_uuid":"def72f3a4cf035009e672315960a27a56dee6781"},"cell_type":"code","source":"# getting rid of type1 column.\n\nfraud = fraud.drop('type1', 1)\nvalid = valid.drop('type1',1)\ndata_new = data_new.drop('type1',1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0d12ec272d4c2cc42c0285d35e46d201238b34c"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"6e44c0b3b845746cb7c3376694a1938011dd2f5e"},"cell_type":"markdown","source":"### 1.4 Looking at Transaction Types\n<a id='TransType'></a>"},{"metadata":{"trusted":false,"_uuid":"967f70be1104b934e5d9ba05d6ee3bbbdd1c0985"},"cell_type":"code","source":"# seeing the counts of transactions by type\nprint(\"Fraud transactions by type: \\n\",fraud.type.value_counts())\nprint(\"\\n Valid transactions by type: \\n\",valid.type.value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4622ec0cf0fd9c663ede0983210563ddea1e30ed"},"cell_type":"markdown","source":"#### Conclusion: \n\nFrom the dataset, it seems that fraud transactions only occur when the transaction type is CASH_OUT or TRANSFER.\n\nSince the dataset is sample of the population, I would've have resampled the data to see if this phenomenon held.\n\nSince I do not have access to the population, I will assume that transaction only occur when transaction type is either CASH_OUT or TRANSFER."},{"metadata":{"trusted":false,"_uuid":"a45da65b70abfe8fb0f60a5bf76103bd20d9f415"},"cell_type":"code","source":"# Subsetting data according to the conclusion above\n# I don't have to subset for the fraud dataset because all of their transaction types are either TRANSFER or CASH_OUT\n\nvalid = valid[(valid[\"type\"] == \"CASH_OUT\")| (valid[\"type\"] == \"TRANSFER\")]\ndata_new = data_new[(data_new[\"type\"] == \"CASH_OUT\") | (data_new[\"type\"] == \"TRANSFER\")]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb37d471495eaa5a38501f30c572e0b8b8e24be7"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"afd8203ae6365bda92ba15e83dec1bc362658a43"},"cell_type":"markdown","source":"### 1.5 Looking balances before and after the transaction\n<a id='bal'></a>"},{"metadata":{"_uuid":"0636a698abb81fc3147be91075a194c3fab24157"},"cell_type":"markdown","source":"Most, if not all, of the observations have errors in calculating the balances before and after the transaction. "},{"metadata":{"trusted":false,"_uuid":"e5e8fed35caa7a80b4d0ced95c99b6c3f5284246"},"cell_type":"code","source":"wrong_orig_bal = sum(data[\"oldbalanceOrg\"] - data[\"amount\"] != data[\"newbalanceOrig\"])\nwrong_dest_bal = sum(data[\"newbalanceDest\"] + data[\"amount\"] != data[\"newbalanceDest\"])\nprint(\"Percentage of observations with balance errors in the account giving money: \", 100*round(wrong_orig_bal/len(data),2))\nprint(\"Percentage of observations with balance errors in the account receiving money: \", 100*round(wrong_dest_bal/len(data),2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f14bcf13fc5cff63929e1e652f5c81c1e7d5a8d9"},"cell_type":"markdown","source":"Almost all of the observations have inaccurately portrayed what happens to the account receiving money and the account sending money.\n\nSome form of complete or partial imputation (filling/replacing missing or wrong values) must happen."},{"metadata":{"_uuid":"b3352c9b9487e1fd8358537f183a6288c38e6436"},"cell_type":"markdown","source":"Here are some hypothesis/assumptions that I will test soon:\n\n1. There are no negative values in this dataset.\n2. The most a given can give is how much is in their account.\n3. The most a receiver should have in their account is the amount given to them in the transaction."},{"metadata":{"trusted":false,"_uuid":"811f651d59cc1c0b810cdf73b8588d7114eaa023"},"cell_type":"code","source":"## Calculating some quantities to justify or reject some assumptions\n\n# flatten the subsetted dataframe of floats into an array of floats\nrelevant_cols = data[[\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\"]].values.flatten()\n# number of observations with negative numbers\nnum_neg_amt = sum(n < 0 for n in relevant_cols)\n# number of observations where the amount given is greater than the amount that is in the giver's account\nnum_amt_oldgiver = sum(data[\"amount\"] > data[\"oldbalanceOrg\"]) \n# number of observations where the amount received is greater than the amount that is in the receiver's account\nnum_amt_newreceiver = sum(data[\"amount\"] > data[\"newbalanceDest\"]) \n\nprint(\"number of observations with negative numbers: \", num_neg_amt)\nprint(\"number of observations where the amount given is greater than the amount that is in the giver's account: \"\n      , num_amt_oldgiver)\nprint(\"number of observations where the amount received is greater than the amount that is in the receiver's account: \"\n      , num_amt_newreceiver)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a9b15118fba7c97382ad7714792a4d3a13b60d5"},"cell_type":"markdown","source":"With these calculations, hypotheses 2 and 3 have been rejected. "},{"metadata":{"trusted":false,"_uuid":"2f5ebba9614f33b0b0ea91c964449a326bb5f432"},"cell_type":"code","source":"# counting number of observations where oldbalanceOrg - amount != newbalanceOrig or newbalanceDest + amount != newbalanceDest\n# Essentially, I am counting the number of observations where the effects of the transactions are not properly reflected\n# the balances of account sending money and the account receiving money.\n\nnum_wrong_bal = (data[\"oldbalanceOrg\"] - data[\"amount\"] != data[\"newbalanceOrig\"]) | (data[\"newbalanceDest\"] + data[\"amount\"] != data[\"newbalanceDest\"])\nprint(\"Percentage of observations with balance errors: \", 100*round(sum(num_wrong_bal)/len(data),2))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a128c06e0f798bae1d813defd52ba952016801da"},"cell_type":"markdown","source":"In fact, **all** observations have balance contain errors.\n\nSince I don't know why these errors are caused, I cannot replace them. \n\nI also don't want to get rid of the variables oldbalanceOrg, newbalanceOrig, newbalanceDest, oldbalanceDest since they might be important in identifying fraudulent transactions from valid transactions.\n\nSo for now, I will them be.\n\nHowever, do these errors differ between fraudulent and valid transactions?"},{"metadata":{"trusted":false,"_uuid":"16cffdae719b47c107766e5c2da0291016cb099d"},"cell_type":"code","source":"# adding features errorBalanceOrg, errorBalanceDest\ndata_new[\"errorBalanceOrg\"] = data_new.newbalanceOrig + data_new.amount - data_new.oldbalanceOrg\ndata_new[\"errorBalanceDest\"] = data_new.oldbalanceDest + data_new.amount - data_new.newbalanceDest\n\n# Subsetting data into observations with fraud and valid transactions:\nfraud = data_new[data_new[\"isFraud\"] == 1]\nvalid = data_new[data_new[\"isFraud\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"79504c6e113c06cd29aee380035a364e04e3a1ce"},"cell_type":"code","source":"print(\"Summary statistics of errorBalanceOrg for fraudulent transactions: \\n\",fraud[\"errorBalanceOrg\"].describe())\nprint(\"\\n Summary statistics of errorBalanceOrg for valid transactions: \\n\",valid[\"errorBalanceOrg\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"065e73093fbb0d2ba6acdfdc04acf0b03f1861b4"},"cell_type":"markdown","source":"From the summary statistics on the errorBalanceOrg, it seems that a large proportion of the data have an error of 0 or close to zero. This is indicated by the fact that the most negative error is -7.450581e-09 or $-7.450581 x 10^{-9}$ which is very small and close to 0, and the 3rd quartile is 0 (that is, about 75% of the data is between -7.450581e-09 and 0). However, there are some large errors, the largest error being 10,000,000.\n\nOn the other hand, for valid transactions, a large proportion of the data have large errors. For instance,\nabout 75% of the data haver errors exceeding 52,613.43 (the first quartile). The largest error is 92,445,520."},{"metadata":{"trusted":false,"_uuid":"a95194c55bff70e8cbacf98df1616c26d368874e"},"cell_type":"code","source":"print(\"Summary statistics of errorBalanceDest for fraudulent transactions: \\n\",fraud[\"errorBalanceDest\"].describe())\nprint(\"\\n Summary statistics of errorBalanceDest for valid transactions: \\n\",valid[\"errorBalanceDest\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f58e01f4709d37660ade135df048a6d5a78d2391"},"cell_type":"markdown","source":"From the summary statistics of the errorBalanceDest variable, the errors are huge in both directions (both fraudulent and valid transactions have large positive and negative errors in the accounts where money has been moved to.)\n\nLet's see what the differences look like when I plot errorBalanceOrg and errorBalanceDest together."},{"metadata":{"trusted":false,"_uuid":"c6de711f5589f9a13b8ad6b6bb7bc49ed409315d"},"cell_type":"code","source":"errors = [\"errorBalanceOrg\", \"errorBalanceDest\"]\nax = plt.subplot()\n\nfplot = fraud.plot(x=\"errorBalanceOrg\",y=\"errorBalanceDest\",color=\"red\",kind=\"scatter\",ax=ax,label=\"Fraudulent transactions\")\nvplot = valid.plot(x=\"errorBalanceOrg\",y=\"errorBalanceDest\",color=\"green\",kind=\"scatter\",\\\n                   alpha=0.01,ax=ax,label=\"Valid transactions\")\nplt.title(\"errorBalanceOrg vs errorBalanceDest\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6846efff6ebcc505416e1effb9c753081788ffb2"},"cell_type":"markdown","source":"It seems that many fraudulent transactions that are found in the top right corner where errorBalanceDest > 0, whereas transactions occur much more often when the errorBalanceDest <= 0. "},{"metadata":{"trusted":false,"_uuid":"2cee9367385053a1051342782b71002d0df068c7"},"cell_type":"code","source":"print(\"Proportion of fraudulent transactions with errorBalanceDest > 0: \", len(fraud[fraud.errorBalanceDest > 0])/len(fraud))\nprint(\"Proportion of valid transactions with errorBalanceDest > 0: \", len(valid[valid.errorBalanceDest > 0])/len(valid))\nprint(\"Proportion of fraudulent transactions with errorBalanceOrg > 0: \", len(fraud[fraud.errorBalanceOrg > 0])/len(fraud))\nprint(\"Proportion of valid transactions with errorBalanceOrg > 0: \", len(valid[valid.errorBalanceOrg > 0])/len(valid))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe077ca819d8d927dee1c145820dfa4a03ab766b"},"cell_type":"markdown","source":"### Conclusion: \n\nThe spread of errors in both the balanceOrg and balanceDest variables are large, however valid transactions are much more likely to have an errorBalanceOrg > 0.\n\nSimilarly, fraudulent transactions are much more likely to have errorBalanceDest > 0 than valid transactions.\n\nIn addition, only valid transactions have errorBalanceDest > 10,000,000\n\nThese distinctions and probably more, make errorBalanceDest and errorBalanceOrg potentially effective features. "},{"metadata":{"_uuid":"3e55d75529888b174f0603fe96da62bdb50616a5"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"8b1419b7c9b32585b4c7c0ebf26f0937bba59264"},"cell_type":"markdown","source":"### 1.6 Another Look at Transaction Types and Account Names\n<a id='AcctandTrans'></a>"},{"metadata":{"_uuid":"deec337aca21953afaddf2b4e4ee5ff27d0d8348"},"cell_type":"markdown","source":"According to the overview of the dataset on kaggle:\n\n\n*This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.*\n\nLet's see if this is reflected in the fraud dataset"},{"metadata":{"trusted":false,"_uuid":"cc09a8621245efe3202d401e6e2e97913706f9d9"},"cell_type":"code","source":"print(\"Fraud transactions by type: \\n\",fraud.type.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c70b925251374537d45a3696afc9a511bef6fee"},"cell_type":"markdown","source":"Clearly, fraudulent transactions exclusively involved cashouts and transfers"},{"metadata":{"trusted":false,"_uuid":"9ab1c91c8ed6c0cd7b1d6c38de362002aae5887d"},"cell_type":"code","source":"pd.DataFrame.head(data_new,13)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a31a17b27e1d232e0a4cb35079c557b1a6f7f118"},"cell_type":"markdown","source":"However, in this sample the account that the money to transferred to tends to not be the account used to make the cashout. \n\n\nLet's test this statement programmatically."},{"metadata":{"trusted":false,"_uuid":"90754ed1b066d6b045703107e0ad68e9d3925c7e"},"cell_type":"code","source":"# separating transfers and cashouts for fraud accounts\n\nfraud_transfer = fraud[fraud[\"type\"] == \"TRANSFER\"]\nfraud_cashout = fraud[fraud[\"type\"] == \"CASH_OUT\"]\n\n# checking if the recipient account of a fraudulent transfer was used as a sending account for cashing out \nfraud_transfer.nameDest.isin(fraud_cashout.nameOrig).any()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7661c37d931634c4cbbb88207cfaeb75562e0719"},"cell_type":"markdown","source":"### Conclusion:\n\nThus in this dataset, for fraudulent transactions, the account that received funds during a transfer was not used at all for cashing out.\n\nIf that is the case, there seems to be no use for nameOrig or nameDest since there seems to be no restrictions on which accounts cashout from fraudulent transactions.\n\nThus, I am omitting the nameOrig and nameDest columns from analysis."},{"metadata":{"trusted":false,"_uuid":"2ba42cadcc6cdacfb9dc1381b3e0a6c303cf6e77"},"cell_type":"code","source":"# getting rid of nameOrig and nameDest column.\nnames = [\"nameOrig\",\"nameDest\"]\nfraud = fraud.drop(names, 1)\nvalid = valid.drop(names,1)\ndata_new = data_new.drop(names,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b1940b825225deca217f91a8213978b28fbcd5b"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"317485c6424fcd06968f32a1d3c2ed6792f432a4"},"cell_type":"markdown","source":"### 1.7 Looking at Flagged Transactions\n<a id='Flag'></a>"},{"metadata":{"_uuid":"23c98e4ca55081610acb3696c6d9258e3347b041"},"cell_type":"markdown","source":"From the overview, the variable isFlaggedFraud is described as transactions that were flagged as fraud.\n\nTo be flagged as fraud, the transaction would have to be fraudulent and involve a transfer of more than 200, 000 units in a specified currency.\n\nWith that in mind, I have some questions. "},{"metadata":{"trusted":false,"_uuid":"c651cfe9a2b6eeeaa6d5e1b2c25f886218b02dd0"},"cell_type":"code","source":"# how many observations were flagged as Fraud?\nflagged = data_new[data_new[\"isFlaggedFraud\"] == 1]\nflagged_correctly = sum(flagged[\"isFraud\"] == 1)\nflagged_wrongly = len(flagged) - flagged_correctly\ntotal = flagged_correctly + flagged_wrongly\nprint(flagged_correctly,\" observations were flagged correctly and \", flagged_wrongly, \\\n      \" observations were flagged wrongly for a total of \", total, \" flagged observations.\")\n\n# how many observations where the transaction is fraudulent, the transaction is a transfer and the amount is greater \n# than 200, 000 are in the dataset\nshould_be_flagged = fraud[(fraud[\"amount\"] > 200000) & (fraud[\"type\"] == \"TRANSFER\")]\nprint(\"number of observations that should be flagged: \",len(should_be_flagged))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcad7a9e8fc5422cdda49c755c3e9af86644e15e"},"cell_type":"markdown","source":"### Conclusion: \n\nIn a modified dataset with more than 2 million observations, a variable that brings attention to only 16 observations is insignificant.\n\nFurthermore, the number of transactions that should have been flagged far exceeds the number of observations that were actually flagged.\n\nIn addition, I am trying to develop a new fraud detection screen that does not depend on a pre-existing fraud detection scheme.\n\nFor that reason, I am omitting the isFlaggedFraud column from the analysis."},{"metadata":{"trusted":false,"_uuid":"9d5941c2138c57991ceae60a9e63bdfc8540d587"},"cell_type":"code","source":"# dropping isFlaggedFraud column from the fraud,valid, and new_data datasets\n\nfraud = fraud.drop(\"isFlaggedFraud\",1)\nvalid = valid.drop(\"isFlaggedFraud\",1)\ndata_new = data_new.drop(\"isFlaggedFraud\",1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7bd8e27dedcfbbf3f6b171d55fa68357d24a263"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"09956f29079ba73497968d29391ef126876f6841"},"cell_type":"markdown","source":"### 1.8 Looking at Time\n<a id='Time'></a>"},{"metadata":{"trusted":false,"_uuid":"29cf4d5f3d71318aa5abf8ea4c26566f4e95fd0e"},"cell_type":"code","source":"# Time patterns\n\nbins = 50\n\nvalid.hist(column=\"step\",color=\"green\",bins=bins)\nplt.xlabel(\"1 hour time step\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of valid transactions over time\")\n\nfraud.hist(column =\"step\",color=\"red\",bins=bins)\nplt.xlabel(\"1 hour time step\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of fraud transactions over time\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f737cccac3fb58e3c5c2166d550ae9087f2d676"},"cell_type":"markdown","source":"There are stark difference between the *step* data between valid and fraud transactions.\n\n1. A large proportion of valid transactions occur between around the 0th and 60th timestep as well as the 110th and 410th time-steps.\n2. The frequency at which fraudulent transactions occur does not seem to change much over time.\n\nHowever the visualizations showcase the number of transactions for each time step over the course of a month.\n\nLet's see what the patterns look like over any particular, day of the week or hour of the day.\n"},{"metadata":{"trusted":false,"_uuid":"f50fe30c6b175e105cbe29e0f1c7cc6af27dfa69"},"cell_type":"code","source":"# getting hours and days of the week\nnum_days = 7\nnum_hours = 24\nfraud_days = fraud.step % num_days\nfraud_hours = fraud.step % num_hours\nvalid_days = valid.step % num_days\nvalid_hours = valid.step % num_hours\n\n# plotting scatterplot of the days of the week, identifying the fraudulent transactions (red) from the valid transactions (green) \nplt.subplot(1, 2, 1)\nfraud_days.hist(bins=num_days,color=\"red\")\nplt.title('Fraud transactions by Day')\nplt.xlabel('Day of the Week')\nplt.ylabel(\"# of transactions\")\n\nplt.subplot(1,2,2)\nvalid_days.hist(bins=num_days,color=\"green\")\nplt.title('Valid transactions by Day')\nplt.xlabel('Day of the Week')\nplt.ylabel(\"# of transactions\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c11c046ad2e9abb6632c2b24009a3d16e7090418"},"cell_type":"markdown","source":"Note: With respect to days, day 0 does not necessarily mean the first day of the week, Sunday. \n\nE.g If day 0 is Wednesday, then day 1 is Thursday, day 2 is Friday and so on...\n\nFrom the graphs above, there is little evidence to suggest that fraudulent transactions occur at particular days of the week.\n\nMuch like valid transactions, fraudulent transactions seem to occur uniformally for each day of the week.\n\nThus I won't make a feature showing what day of the week that the transaction occured."},{"metadata":{"trusted":false,"_uuid":"c7b9dbde7beb484cb41f9a1199b29500d425a0ac"},"cell_type":"code","source":"plt.subplot(1, 2, 1)\nfraud_hours.hist(bins=num_hours, color=\"red\")\nplt.title('Fraud transactions by Hour')\nplt.xlabel('Hour of the Day')\nplt.ylabel(\"# of transactions\")\n\n\nplt.subplot(1, 2, 2)\nvalid_hours.hist(bins=num_hours, color=\"green\")\nplt.title('Valid transactions by Hour')\nplt.xlabel('Hour of the Day')\nplt.ylabel(\"# of transactions\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45430dbcc74b5994978fbac0cc866f505a6086d6"},"cell_type":"markdown","source":"### Conclusion:\n\nNote: With respect to days, hour 0 does not necessarily mean 1am in the morning. \n\nE.g If hour 0 is 9am, then hour 1 is 10 am, hour 2 is 11am and so on...\n\nFrom the graphs above, there is strong evidence to suggest that from hour 0 to hour 9 (inclusive) valid transactions very seldom occur. On the other hand, fraudulent transactions still occur at similar rates to any hour of the day outside of hours 0 to 9 (inclusive).\n\nIn response to this, I will create another feature HourOfDay, which is the step column with each number taken to modulo 24."},{"metadata":{"trusted":false,"_uuid":"455b8e8ab3b1ead3de4b8d01ca0e52cb49d56d1e"},"cell_type":"code","source":"dataset1 = data_new.copy()\n\n\n# adding feature HourOfDay to Dataset1 \ndataset1[\"HourOfDay\"] = np.nan # initializing feature column\ndataset1.HourOfDay = data_new.step % 24\n\n\nprint(\"Head of dataset1: \\n\", pd.DataFrame.head(dataset1))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce494e001c7c1d78ca8feea1544ddc27771879c0"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"061fbd106c2033815f4fd26804d0095e4168b8cb"},"cell_type":"markdown","source":"### 1.9 Looking at Amounts Moved in Transactions\n<a id='Amt'></a>"},{"metadata":{"trusted":false,"_uuid":"68cd168a839046714b8429831eb6ce3e27cd7343"},"cell_type":"code","source":"# Seeing summary statistics of the data\n\nprint(\"Summary statistics on the amounts moved in fraudulent transactions: \\n\",pd.DataFrame.describe(fraud.amount),\"\\n\")\nprint(\"Summary statistics on the amounts moved in valid transactions: \\n\", pd.DataFrame.describe(valid.amount),\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e56ad2b677c9961e1b63c945e4031e9d6f05d024"},"cell_type":"markdown","source":"It seems that during fraudulent transactions, the amount moved is capped at 10 million currency units.\n\nWhereas for valid transactions, the amount moved is capped at about 92.4 million currency units.\n\nwhen plotting time-steps against amount moved we get this plot...\n"},{"metadata":{"trusted":false,"_uuid":"8a27b5f0a6849e6342254f843c843c85c5cb09cf"},"cell_type":"code","source":"# plotting overlayed step vs amount scatter plots\n\nalpha = 0.3\nfig,ax = plt.subplots()\nvalid.plot.scatter(x=\"step\",y=\"amount\",color=\"green\",alpha=alpha,ax=ax,label=\"Valid Transactions\")\nfraud.plot.scatter(x=\"step\",y=\"amount\",color=\"red\",alpha=alpha,ax=ax, label=\"Fraudulent Transactions\")\n\nplt.title(\"1 hour timestep vs amount\")\nplt.xlabel(\"1 hour time-step\")\nplt.ylabel(\"amount moved in transaction\")\nplt.legend(loc=\"upper right\")\n\n# plotting a horizontal line to show where valid transactions behave very differently from fraud transactions\n\nplt.axhline(y=10000000)\nplt.show()\n\n\nprint(\"Proportion of transactions where the amount moved is greater than 10 million: \", \\\n      len(data_new[data_new.amount > 10000000])/len(data_new))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52211c3fc4eea57255744989f6ce4faa3ec42f3b"},"cell_type":"markdown","source":"### Conclusion:\n\nOnly valid transaction involved amounts larger than 10,000,000, however these transactions make up less than 0.01% of the relevant data.\n\nWhen the amounts moved is less than 10,000,000 there doesn't seem to be a large difference fraudulent and valid transactions.\n\nI leave the variable amount as is without creating a feature out of it."},{"metadata":{"trusted":false,"_uuid":"08bc603d201171aeaaa1c1c9fd9a19cd7330093f"},"cell_type":"code","source":"# finalizing dataset\ndataset = dataset1.copy() # unchanged dataset1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15a9d64e90ef006bfc712dec01366f0d38dc1715"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"13ed100736440c745b4174188c577fab4aad9bc4"},"cell_type":"markdown","source":"## 2. Pre-processing Data\n<a id='Preprocess'></a>"},{"metadata":{"_uuid":"0dee8617423d3aec4f4a3e869664e0def136ff08"},"cell_type":"markdown","source":"### 2.1 Handling Categorical Variables\n<a id='Cat'></a>\n\nNote that many algorithms require that all elements used in the computation are numbers.\n\nFor that reason, the categorical variables encoded as string must be encoded as numbers. Since there is no \"order\"/\"hierarchy\" in the type variable, the method I will use to numerically encode categorical variables is called 1 hot encoding.\n\nOne-Hot encoding involves creating indicator variables for each category in a categorical variable.\n\nIf an observation is part of a particular category (e.g. the transaction type is CASH_OUT), the indicator variable associated with the category would be 1. If it isn't part of a particular category, then the indicator variable associated with that category would be 0.\n"},{"metadata":{"trusted":false,"_uuid":"92f5bfb1f6dd1d8291b1112d2df097413cd03378"},"cell_type":"code","source":"# getting one-hot encoding of the 'type' variable\n\ndataset = pd.get_dummies(dataset,prefix=['type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"82a78ef5ec83d05597051dd4a551dec91f93cd28"},"cell_type":"code","source":"pd.DataFrame.head(dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcf78dd0b164fef2e492248dbedff35f4d28845b"},"cell_type":"markdown","source":"## 2.2 Splitting and Standardizing Data.\n<a id='Split'></a>\nSimilarly, many, if not all, machine learning algorithms perform better when the data is standardized/normalized (when all values are between 0 and 1 inclusive).\n\nWe will do this to standardize the data without standardizing the target variable isFraud.\n\nAdditionally, we will also split the data up into training sets and testing sets. A common split is to separate 80% of the data as the training set and the rest as the testing set. However we will rely on the \"default\" split which is 75% of the data is used as the training set, 25% is used as the testing set.\n"},{"metadata":{"trusted":false,"_uuid":"0018683e4c757a67a7b3fde1be8d9a533e992233"},"cell_type":"code","source":"# Setting random_state and seed so that the training/testing splits and model results are reproducible\nRandomState = 42\nseed(21)\n\n\n# 42 is used often due to Hitchhiker's Guide to the Galaxy, I will use a number that a far smaller group may understand.\n# Not that the actual number doesn't matter and is only used to make sure results are reproducible.\n# creating training and testing sets\nX = dataset.drop(\"isFraud\",1)\ny = dataset.isFraud\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n    \n# Normalizing data so that all variables follow the same scale (0 to 1)\nscaler = StandardScaler()\n\n# Fit only to the training data\nscaler.fit(X_train)\n\n# Now apply the transformations to the data:\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0902552e5c64425d0c869634d5aed5ba7651c33"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"9cb20c705c396b934f165e81b0a732ad34bb50fb"},"cell_type":"markdown","source":"## 3 Model Selection\n<a id='Models'></a>"},{"metadata":{"_uuid":"735f12cde3cb63edca0551798313b9e174495b16"},"cell_type":"markdown","source":"### 3.1 Model 1: Artificial Neural Networks\n<a id='Model-1'></a>\nSince correctly predicting fraudulent and valid transactions is the main goal and my dataset is fairly large (> 2 million rows), I think that Neural Networks would be a good choice. \n\nIn particular, I will be using a Multilayered Perceptron.\n\nMulti Layer perceptron (MLP) is a feedforward neural network with at least 1 layer between the input and output layer. Data is trained from the input layer up until the output layer ([Techopedia](https://www.techopedia.com/definition/20879/multilayer-perceptron-mlp])).\n\n"},{"metadata":{"trusted":false,"_uuid":"4b215f4401d04dfdc8d719ccf7168caa849b5264"},"cell_type":"code","source":"ncols = len(X.columns)\nhidden_layers = (ncols,ncols,ncols)\nmax_iter = 1000\nMLP = MLPClassifier(hidden_layer_sizes=hidden_layers,max_iter=1000,random_state=RandomState)\n\n# training model\nMLP.fit(X_train,y_train)\n    \n# evaluating model on how it performs on balanced datasets\npredictionsMLP = MLP.predict(X_test)\nCM_MLP = confusion_matrix(y_test,predictionsMLP)\nCR_MLP = classification_report(y_test,predictionsMLP)\nfprMLP, recallMLP, thresholdsMLP = roc_curve(y_test, predictionsMLP)\nAUC_MLP = auc(fprMLP, recallMLP)\n    \nresultsMLP = {\"Confusion Matrix\":CM_MLP,\"Classification Report\":CR_MLP,\"Area Under Curve\":AUC_MLP}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bdf5ea8b0b941bb72519a7ccd306172fafef2ff3"},"cell_type":"code","source":"# showing results from Multilayered perceptrons developed from each dataset\nfor measure in resultsMLP:\n    print(measure,\": \\n\",resultsMLP[measure])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"288e2335a1403d12178a9677c01fd2907e7c7b5b"},"cell_type":"markdown","source":"In the context of fraud detection the performance of the Neural Network isn't terrible, but it isn't great either. The loss is performance is very likely due to the phenomenon that Neural Networks perform worse when the data is imbalanced. When data is imbalanced, Neural Networks and many other models trained on the data tend to be very biased towards the *majority class*. In our case, the majority class are valid transactions.  \n\nThis model will be the benchmark that I will compare other individual models against.\n\nThe next few models will be generated from methods that are well-known for handling imbalanced data effectively."},{"metadata":{"_uuid":"acdaf9722037b647f940b2d7044971c4c01c3e1e"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"83a943645260f03b38334868a505c86bd4ba60b3"},"cell_type":"markdown","source":"### 3.2 Model 2: Random Forest.\n<a id='Model-2'></a>\n\nA random forest is an algorithm that generates several decisions trees and pools the results of each tree to make a more robust prediction ([Eulogio, 2017](https://www.datascience.com/resources/notebooks/random-forest-intro)). \n\nAnother great thing about Random Forest is that I can assign weights to each class to reduced the bias of the model towards the majority class, in this case valid transaction."},{"metadata":{"trusted":false,"_uuid":"678d5e9f3078534be720e2d2b19d50815bfafe5b"},"cell_type":"code","source":"# Train model\nparametersRF = {'n_estimators':15,'oob_score':True,'class_weight': \"balanced\",'n_jobs':-1,\\\n                 'random_state':RandomState}\nRF = RandomForestClassifier(**parametersRF)\nfitted_vals = RF.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsRF = RF.predict(X_test)\n \n     \n# Evaluating model\nCM_RF = confusion_matrix(y_test,predictionsRF)\nCR_RF = classification_report(y_test,predictionsRF)\nfprRF, recallRF, thresholdsRF = roc_curve(y_test, predictionsRF)\nAUC_RF = auc(fprRF, recallRF)\n\nresultsRF = {\"Confusion Matrix\":CM_RF,\"Classification Report\":CR_RF,\"Area Under Curve\":AUC_RF}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"598653e9bd99248e6cc2cc068443fdc62b234ee9"},"cell_type":"code","source":"# showing results from Random Forest\n\nfor measure in resultsRF:\n    print(measure,\": \\n\",resultsRF[measure])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5cc27f6d798851e5505597deb28afb4275ffdc"},"cell_type":"markdown","source":"As expected, the Random Forest performs much better than the Neural Networks. Instead of crowning this model as the best model, let's try another model known for performing well in imbalanced datasets."},{"metadata":{"_uuid":"b6cc2474da2995d440820cd432c95645248973f7"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"ab838125e069d1dc87be0551f33baee37c8a788b"},"cell_type":"markdown","source":"### 3.3 Model 3: e**X**treme **G**radient **B**oosting Trees (or XGB trees for short)\n<a id='Model-3'></a>\n\nThis algorithm is well known for being used in imbalanced datasets. Similar to Random Forests, the algorithm generates several decision trees and pooling the results. \n\nHowever,instead of generating multiple full blown decision trees in parallel and pooling the results, it generates multiple trees formed by weak learners sequentially and then it pools the results ([Ravanshad, 2018](https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)).\n\nAs with Random Forests, I could set weights such that the model is less biased towards the majority class."},{"metadata":{"trusted":false,"_uuid":"69142693de84287f4d633a3c57ab6ad730eea378"},"cell_type":"code","source":"# Train model\nweights = (y == 0).sum() / (1.0 * (y == 1).sum()) # for unbalanced datasets, these weights are recommended\nparametersXGB = {'max_depth':3,'scale_pos_weight': weights,'n_jobs':-1,\\\n                 'random_state':RandomState,'learning_rate':0.1}\nXGB = XGBClassifier(**parametersXGB)\n    \nfitted_vals = XGB.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsXGB = XGB.predict(X_test)\n \n     \n# Evaluating model\nCM_XGB = confusion_matrix(y_test,predictionsXGB)\nCR_XGB = classification_report(y_test,predictionsXGB)\nfprXGB, recallXGB, thresholds_XGB = roc_curve(y_test, predictionsXGB)\nAUC_XGB = auc(fprXGB, recallXGB)\nresultsXGB = {\"Confusion Matrix\":CM_XGB,\"Classification Report\":CR_XGB,\"Area Under Curve\":AUC_XGB}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5e260b5509a1af6ad4e3f07e503ab84d8198c4d9"},"cell_type":"code","source":"# showing results from Extreme Gradient Boosting\nfor measure in resultsXGB:\n    print(measure,\": \\n\",resultsXGB[measure],\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bcd4b063eee3015b2736bd751e31bae07c1e3c3"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"9df90a5687f73937dd0a13438a7730ff8ce74938"},"cell_type":"markdown","source":"### 3.4 Comparing Performances\n<a id='Visuals'></a>"},{"metadata":{"_uuid":"26aba1cf9b0bf36071bebb5fad64a828d7138848"},"cell_type":"markdown","source":"Clearly, the random forest and the extreme gradient boosted trees performed better than the neural networks, but which one is better?\n\n Instead of comparing all of the metrics from a purely statistical or mathematical point of view, let's look at them at give some practical interpretation.\n\nFirst off, let's compare their confusion matrices.\n\nAdditionally, instead of comparing the number of correct predictions, let us compare the number of wrong predictions."},{"metadata":{"trusted":false,"_uuid":"57b0232c8e23176ddc318c71a0b10606bc7831a3"},"cell_type":"code","source":"print(\"Number of valid transactions labelled as fraudulent by Random Forest: \\n\", CM_RF[0,1])\nprint(\"Number of valid transactions labelled as fraudulent by XGB trees: \\n\", CM_XGB[0,1])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ddc6ba5634ec2e894bb8cda1680117e5b7a51a"},"cell_type":"markdown","source":"On the basis on limiting the amount of valid transaction labelled as fraudulent, the Random Forest performed better."},{"metadata":{"trusted":false,"_uuid":"6c7bb568a4a8f736fcbd1e19af4da38cc9c75467"},"cell_type":"code","source":"print(\"Number of fraud transactions labelled as valid by Random Forest: \\n\", CM_RF[1,0])\nprint(\"Number of fraud transactions labelled as valid by XGB trees: \\n\", CM_XGB[1,0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3977616fdb4b5227c67ed70fa62f2920cec70355"},"cell_type":"markdown","source":"On the basis on limiting the amount of fraudulent transactions labelled as fraudulent, the XGB trees performed better."},{"metadata":{"_uuid":"e31feceb286ecdabf0d8a82acc0d54ed2feeb7bf"},"cell_type":"markdown","source":"Based purely on the results on the confusion matrix, the better model is decided by which model incurs the lowest costs.\n\nIf the combined cost of mislabelling over 100 more valid transactions as fraudulent exceeds the cost of mislabelling a few more fraudulent transactions as valid then the random forest would be a better model.\n\nOtherwise, the Extreme Gradient Boosted model would be superior.\n\nSome of the other metrics tracked (precision, recall, f1-score which are found in the classification report) will convey the same information that is offered by the confusion matrix.\n\nSo what if we compared the classification reports?"},{"metadata":{"trusted":false,"_uuid":"2aa5653b92bdd3b36bbd908cf456471c9563323e"},"cell_type":"code","source":"print(\"Note: scores in the same vertical level as 0 are scores for valid transactions. \\n \\\n      Scores in the same vertical level as 1 are scores for fraudulent transactions. \\n\")\nprint(\"Classification Report of Random Forest: \\n\", CR_RF)\nprint(\"Classification Report of XGB trees: \\n\", CR_XGB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"842d31876ff45673688e26ee4c0222096f4f786a"},"cell_type":"markdown","source":"While the recall scores for both models are identical, the Random Forest performed slightly better in terms of their precision score for fraudulent transactions. \n\nThis means that there are considerably less false positives (identifying valid transactions as fraudulent) in the Random Forest than in the XGB model. This makes sense given what we've seen in their confusion matrices (a few valid transactions labelled as fraudulent by Random Forest, compared to over 100 by the XGB model). \n\nBased on the classification report, the Random Forest is superior."},{"metadata":{"_uuid":"3ab0138ddff8134e6dac56397a19b0b881a772c3"},"cell_type":"markdown","source":"What about AUC's (area under the curve)?\n\nThe only reason why I computed AUC's is because it is a popularly used metric to measure performance in kaggle competitions. \nThe curve in Area Under **Curve** is a plot of the true positive rates (in our case, the proportion of valid transactions labelled as valid) against the false positive rate (in our case, the proportion of fraudulent transactions labelled as valid). The curve is also known as the Receiver Operating Characteristic Curve or ROC.\n\nThe ideal AUC is then 1 (all transactions predicted as valid are actually valid). \n\n"},{"metadata":{"trusted":false,"_uuid":"0ff09fd1d1bad6c01b3d8af81b619080abe21baa"},"cell_type":"code","source":"print(\"\\nReceiver Operating Characteristic Curves for Random Forests and Extreme Gradient Boosted Trees: \\n\")\nplt.subplot(1, 2, 1)\nplt.plot(fprRF, recallRF, color='purple', label='ROC curve (area = %0.2f)' % AUC_RF)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([-0.01, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC (Random Forest)')\nplt.legend(loc=\"lower right\")\n\n\nplt.subplot(1, 2, 2)\nplt.plot(fprRF, recallRF, color='green', label='ROC curve (area = %0.2f)' % AUC_RF)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([-0.01, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC (XGB)')\nplt.legend(loc=\"lower right\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nAUC of Random Forest: \\n\", AUC_RF)\nprint(\"\\nAUC of XGB trees: \\n\", AUC_XGB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03338f897cea53d1139d9112bbcb9710976d4cab"},"cell_type":"markdown","source":"While the AUC of the Extreme Gradient Boosted Trees is slightly greater than the AUC of the Random Forests. For all practical purposes, they are essentially the same."},{"metadata":{"_uuid":"7e136452483763a171201bf7040efacde9bfca9f"},"cell_type":"markdown","source":"Overall, I would deem the Random Forest as the superior choice because I think the cost of resolving over a 100 valid transactions labelled as fraudulent would exceed the cost of resolving a handful more fraudulent transactions that have been passed off as valid.\n\nWith that said, this may not true in actual companies. The best decision would be to consult people with experience dealing with mislabelled transactions."},{"metadata":{"_uuid":"7c4d3c39a7310a9f72c5ae1d29c09d58f009f42b"},"cell_type":"markdown","source":"Black-box/Non-parametric methods (like Neural Networks, Random Forests, Extreme Gradient Boosted Trees) are known to not be interpretable (due to having a lack of an equation to interpret from).\n\nNonetheless, let's take a look at what features ended up being the most important in classifying transactions."},{"metadata":{"trusted":false,"_uuid":"6720ac28c7ad37476acc7fefd046ece99a6ce398"},"cell_type":"code","source":"x = np.arange(ncols)\n\n# getting importances of features\nimportances = RF.feature_importances_\n\n# getting the indices of the most important feature to least important\nsort_ind = np.argsort(importances)[::-1]\nplt.figure(figsize=(18,7))\nplt.bar(x, importances[sort_ind])\nplt.xticks(x,tuple(X.columns.values[sort_ind]))\nplt.title(\"Important Features: Greatest to Least\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60154850c3e0762f37b81bb9aae0e6c5d8ef43b8"},"cell_type":"markdown","source":"It seems that errorBalanceOrg ended by the most important feature by far for classifying transactions followed by oldBalanceOrg and newBalanceOrig.  "},{"metadata":{"_uuid":"0cf60817daf32e021bbcfe9c7b4426e10593210d"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"},{"metadata":{"_uuid":"e3f74eaa968a3ce2cd29fe37bc1ca1074c09ccaa"},"cell_type":"markdown","source":"## 4 Final Remarks\n<a id='Final'></a>"},{"metadata":{"_uuid":"383d6e2d36b0e04cddb19419b2b1c1f8c09ad432"},"cell_type":"markdown","source":"From the exploring the dataset, we uncovered patterns that allowed us to construct important features and discard useless ones. \n\nWe applied a few popular machine learning algorithms and saw that methods that involved generating multiple decisions trees and pooling their results together performed better than Multi-Layered Perceptrons (a type of Neural Network). While it may seem that Neural Networks are unsuitable for unbalanced datasets, in some occasions, techniques such as SMOTE, oversampling and undersampling can resolve such issues.  \n\nFinally, between the Random Forests and the Extreme Gradient Boosting, practical considerations were used to decide that the Random Forests were a better model.\n\nIn particular, Random Forests were better because we thought that the cost of dealing with over a 100 wrongly labeled valid transactions is more expensive than the cost of dealing with a few additional fraudulent transactions.\n\nWorking with this dataset was a lot of work and a lot of fun. I learned a lot about exploring data and when to apply/not apply certain methods.\n\nThanks for reading! . "},{"metadata":{"_uuid":"a496482363cb386a06545f8b7fb91f173ef954d4"},"cell_type":"markdown","source":"<a href='#top'>go back to top</a>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}